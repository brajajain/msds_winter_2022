{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 5 - Naive Bayes\n",
    "## Raja Jain | 2022-02-27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from pprint import pprint as pp\n",
    "import docx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sweetviz as sv\n",
    "from matplotlib import pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pandarallel import pandarallel\n",
    "from scipy.stats import norm\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split\n",
    "from sklearn.naive_bayes import BernoulliNB, CategoricalNB, ComplementNB, GaussianNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "\n",
    "def getText(filename):\n",
    "    doc = docx.Document(filename)\n",
    "    fullText = []\n",
    "    for para in doc.paragraphs:\n",
    "        fullText.append(para.text)\n",
    "    return \"\\n\".join(fullText)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMP 4448: Data Science Tools II\t\t\t\t\t\t          Assignment 5\n",
      "Directions: Do this assignment in Jupyter Notebook and provide screenshots of code and output in this word document wherever required. You will upload this word document containing screenshots of code and answers as well as your Jupyter Notebook to Canvas. All assignments will be submitted and graded through canvas and grades will be transferred to the 2U platform. \n",
      "Goal: The goal of this assignment is to give you the opportunity to implement the Naïve Bayes Algorithm from scratch as well as using tools built into sklearn. \n",
      "Packages: Core packages you may need for this assignment include numpy, pandas, sklearn, matplotlib.pyplot and/or seaborn, nltk, string, and re. \n",
      "Note: In sklearn, there are different types of Naive Bayes constructors for fitting Naïve Bayes models, dependent on the nature of the data. For example:\n",
      "MultinomialNB() is used for text classification when data is represented as feature vectors. \n",
      "ComplementNB() is an adaptation of the standard  MultinomialNB() for imbalance data.  \n",
      "GaussianNB() is used if the features are assumed numerical and are assumed to follow a Gaussian or normal distribution. \n",
      "BernoulliNB() is used when each feature follows a Bernoulli distribution. That is, the data or all features are binary with values 0 or 1. \n",
      "CategoricalNB() is used when each feature has its own categorical distribution.\n",
      "()\n",
      "Question 1:\n",
      "Upload the income_evaluation_cat.csv provided on canvas.  The features in this data include workclass, education, race, and gender. The output variable is income and contains two categorical values (<=50k or >50k) indicating whether the income of an individual is less than/equal to $50,000 or greater than $50,000 respectively. Print the unique values of each variable in this data. \n",
      "\n",
      "\n",
      "You will implement Naïve Bayes from scratch using Bayes’ rule. You can do your calculations in Python, but you would not use the sklearn package. Suppose that all the income_evaluation_cat.csv data you uploaded is the training data , classify a test instance, X = [“Private”, “Bachelors”, “White”, “Female”] into the class income<=50 or income>50k. You need to compute the posterior probabilities P(income<=50/X) and P(income>50k/X), then print the class with the greater posterior probability as the predicted class. You don’t need to define a function, but you could if you want. \n",
      "\n",
      "\n",
      "\n",
      "Preprocess or transform the features in the income_evaluation_cat.csv data using an appropriate scaler in sklearn. You don’t need to transform the output variable; it should still work fine in a text format. \n",
      "\n",
      "\n",
      "\n",
      "Randomly split the transformed input data and the output data into X_train, y_train, X_test and y_test using tools in sklearn. \n",
      " \n",
      "Use an appropriate Naïve Bayes constructor in sklearn to construct and fit a Naïve Bayes model on the training data, then use the model to compute the accuracy score of the training and test set. \n",
      "\n",
      "\n",
      "Question 2 \n",
      "Upload the income_evaluation_continuous.csv data provided on canvas.  The features in this data include age, education_num, and hours_per_week. The output variable is income and contains two categorical values (<=50k or >50k) indicating whether the income of an individual is less than/equal to $50,000 or greater than $50,000 respectively. Compute the mean and standard deviation of each input variable such that the results are presented on the same table or data frame. You can call the .apply() function on the pandas DataFrame.  \n",
      " \n",
      "\n",
      "You will implement Naïve Bayes from scratch using Bayes’ rule. Assume that all the features or input variables follow a normal distribution. You can do your calculations in Python, but you would not use the sklearn package. You can use the density function inside the stats module in the SciPy package. Given that all the income_evaluation_continuous.csv data you uploaded is the training data , classify a test instance, X = [30, 10, 45], into the class income<=50 or income>50k. You need to compute the posterior probabilities P(income<=50/X) and P(income>50k/X), then print the class with the greater posterior probability as the predicted class. You don’t need to define a function, but you could if you want. \n",
      "\n",
      "\n",
      "\n",
      "Preprocess or transform the features in the income_evaluation_cont.csv data using an appropriate scaler in sklearn. You don’t need to transform the output variable; it should still work fine in a text format. \n",
      "N/A\n",
      "Randomly split the input and output data into X_train, y_train, X_test and y_test using tools in sklearn. \n",
      " \n",
      "\n",
      "Use an appropriate Naïve Bayes constructor in sklearn to construct and fit a Naïve Bayes model on the training data, then use the model to compute the accuracy score of the training and test set. \n",
      "Question 3:\n",
      "You will now implement a Naïve Bayes for text classification to detect fake or true news. \n",
      "Upload the True.csv data provided on canvas into Python. You will create a new data frame by selecting the “title” and “text” columns, then, adding a new column called “news_type” where all the values on this new column are “True”. So, your new data frame should have three columns; “title”, “text” and “news_type”. \n",
      " \n",
      "\n",
      "Upload the Fake.csv data provided on canvas into Python. You will create a new data frame by selecting the “title” and “text” columns, then, adding a new column called “news_type” where all the values on this new column are “Fake”. So, your new data frame should have three columns; “title”, “text” and “news_type”.\n",
      "Merge the data frame in a) and b) so that one of the data frames is stacked vertically on top of the other. Combine the text in the “title” and “text” columns of the merged data frame into another column called “news”. Drop the “title” and “text” columns so that your final data frame is has only two columns, “news” and “news_type”. Print the first five rows of your final data frame. \n",
      "\n",
      "Preprocess your data by cleaning the textual data in the “news” column and removing the stop words, special characters, punctuations, etc especially at the beginning and end of each word. You can display any messy news text before you clean the data, then display the messy news text again after cleaning the data to see if your data cleaning worked well. Also, drop instances where the news text is less than 50 words. \n",
      "\n",
      "\n",
      "Transform the input text data into feature vectors where the entries of the feature vectors are term-frequency-inverse-document-frequency. Use the TfidfVectorizer() in sklearn. \n",
      "\n",
      "\n",
      "Spit the feature vectors and the output variable into into X_train, y_train, X_test and y_test, you can let the test set be 30% of the entire data. \n",
      "\n",
      "\n",
      "\n",
      "Fit an appropriate Naïve Bayes model and compute the training and test accuracy of the model. Is there overfitting?\n",
      "\n",
      "\n",
      "Fit a Naïve Bayes using cross validation and print the average cross validation score as well as the standard deviation of the cross-validation scores. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Select some hypermeters of your choice and tune using the grid search cross validation. Use some other hyperparameters than those used in class examples. \n",
      "\n",
      "\n",
      "\n",
      "4) Mini Project\n",
      "Find some text data of your own choice, it could be labelled tweets, etc. \n",
      "Your dataset should have at least 200 instances, and if there are several columns of text, you can choose to merge the text columns into a single text column. Each text instance should have at least 60 words. \n",
      "Clean the data, split the data, transform the data to a representation suitable for your algorithm, build your model and evaluate the model. Tune some parameters of interest and write a short report about what problem your mini project is trying to address, the description of your data, the choice of algorithm used, the performance of your algorithm, overfitting, the choice of hyperparameters tunned, then your recommendation or conclusion (imagine you were trying to recommend this algorithm to a stakeholder, and you need this report to include important and persuasive elements). Your report could be in one or two paragraphs and should include relevant code and output at the end. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = getText(\"Assignment 5 - Naive Bayes.docx\")\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " -------------------- column: workclass --------------------\n",
      "Private             22696\n",
      "Self-emp-not-inc     2541\n",
      "Local-gov            2093\n",
      "?                    1836\n",
      "State-gov            1298\n",
      "Self-emp-inc         1116\n",
      "Federal-gov           960\n",
      "Without-pay            14\n",
      "Never-worked            7\n",
      "Name: workclass, dtype: int64\n",
      "\n",
      " -------------------- column: education --------------------\n",
      "HS-grad         10501\n",
      "Some-college     7291\n",
      "Bachelors        5355\n",
      "Masters          1723\n",
      "Assoc-voc        1382\n",
      "11th             1175\n",
      "Assoc-acdm       1067\n",
      "10th              933\n",
      "7th-8th           646\n",
      "Prof-school       576\n",
      "9th               514\n",
      "12th              433\n",
      "Doctorate         413\n",
      "5th-6th           333\n",
      "1st-4th           168\n",
      "Preschool          51\n",
      "Name: education, dtype: int64\n",
      "\n",
      " -------------------- column: race --------------------\n",
      "White                 27816\n",
      "Black                  3124\n",
      "Asian-Pac-Islander     1039\n",
      "Amer-Indian-Eskimo      311\n",
      "Other                   271\n",
      "Name: race, dtype: int64\n",
      "\n",
      " -------------------- column: gender --------------------\n",
      "Male      21790\n",
      "Female    10771\n",
      "Name: gender, dtype: int64\n",
      "\n",
      " -------------------- column: income --------------------\n",
      "<=50K    24720\n",
      ">50K      7841\n",
      "Name: income, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Question 1:\n",
    "# Upload the income_evaluation_cat.csv provided on canvas.  The features in this data include workclass, education, race, and gender. The output variable is income and contains two categorical values (<=50k or >50k) indicating whether the income of an individual is less than/equal to $50,000 or greater than $50,000 respectively. Print the unique values of each variable in this data.\n",
    "\n",
    "income_evaluation_cat = pd.read_csv(\"income_evaluation_cat.csv\", skipinitialspace=True)\n",
    "# sanitize column names\n",
    "income_evaluation_cat.columns = income_evaluation_cat.columns.str.lower().str.strip().str.replace(\" \", \"_\")\n",
    "\n",
    "\n",
    "for col in income_evaluation_cat.columns:\n",
    "    print(\"\\n\", \"--\" * 10, \"column:\", col, \"--\" * 10)\n",
    "    print(income_evaluation_cat[col].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Input: {'workclass': 'Private', 'education': 'Bachelors', 'race': 'White', 'gender': 'Female'} \n",
      " Class Probabilities: {'<=50K': 0.7919784735489185, '>50K': 0.20802152645108152} \n",
      " Predicted Class: <=50K\n",
      " Input: {'workclass': 'Self-emp-inc', 'education': 'Doctorate', 'race': 'White', 'gender': 'Male'} \n",
      " Class Probabilities: {'<=50K': 0.05529590397879263, '>50K': 0.9447040960212074} \n",
      " Predicted Class: >50K\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'>50K'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You will implement Naïve Bayes from scratch using Bayes’ rule. You can do your calculations in Python, but you would not use the sklearn package. Suppose that all the income_evaluation_cat.csv data you uploaded is the training data, classify a test instance, X = [“Private”, “Bachelors”, “White”, “Female”] into the class income<=50 or income>50k. You need to compute the posterior probabilities P(income<=50/X) and P(income>50k/X), then print the class with the greater posterior probability as the predicted class. You don’t need to define a function, but you could if you want.\n",
    "\n",
    "\n",
    "class NaiveBayesCategorical:\n",
    "    def __init__(self, data: pd.DataFrame = income_evaluation_cat, target_column: str = \"income\"):\n",
    "        self.data = data\n",
    "        self.target_column = target_column\n",
    "        self.freq_dict = self.generate_frequency_dictionary()\n",
    "        self.len_data = len(self.data)\n",
    "        self.unique_targets = self.data[self.target_column].unique()\n",
    "\n",
    "    def generate_frequency_dictionary(self):\n",
    "        freq_dict = {}\n",
    "        for col in self.data.columns:\n",
    "            data = pd.crosstab(self.data[col], self.data[self.target_column], margins=True)\n",
    "            freq_dict[col] = data.to_dict(orient=\"index\")\n",
    "        return freq_dict\n",
    "\n",
    "    def predict(self, X: list = None, verbose: bool = False):\n",
    "        X = {col: X[i] for i, col in enumerate(self.data.drop(columns=self.target_column).columns)}\n",
    "\n",
    "        probability_dict = {k: 1 for k in self.unique_targets}\n",
    "\n",
    "        for k, v in X.items():\n",
    "            count_predictor = self.freq_dict[k][v][\"All\"]\n",
    "            probability_predictor = count_predictor / self.len_data\n",
    "\n",
    "            for target_class in self.unique_targets:\n",
    "\n",
    "                count_class = self.freq_dict[k][\"All\"][target_class]\n",
    "                probability_class = count_class / self.len_data\n",
    "\n",
    "                count_predictor_in_class = self.freq_dict[k][v][target_class]\n",
    "                probability_predictor_in_class = count_predictor_in_class / count_class\n",
    "                # print(\n",
    "                #     k, v, target_class, probability_predictor, probability_class, probability_predictor_in_class\n",
    "                # )\n",
    "                probability_dict[target_class] *= probability_predictor_in_class\n",
    "\n",
    "        for target_class in probability_dict.keys():\n",
    "            probability_dict[target_class] *= self.freq_dict[self.target_column][\"All\"][target_class] / self.len_data\n",
    "\n",
    "        sum_probs = sum(probability_dict.values())\n",
    "        for target_class in probability_dict.keys():\n",
    "            probability_dict[target_class] /= sum_probs\n",
    "\n",
    "        predicted_class = max(probability_dict, key=probability_dict.get)\n",
    "        if verbose:\n",
    "            print(\n",
    "                \" Input:\",\n",
    "                X,\n",
    "                \"\\n\",\n",
    "                \"Class Probabilities:\",\n",
    "                probability_dict,\n",
    "                \"\\n\",\n",
    "                \"Predicted Class:\",\n",
    "                predicted_class,\n",
    "            )\n",
    "        return predicted_class\n",
    "\n",
    "\n",
    "prediction_instance = [\"Private\", \"Bachelors\", \"White\", \"Female\"]\n",
    "\n",
    "nb_categorical = NaiveBayesCategorical(income_evaluation_cat, \"income\")\n",
    "nb_categorical.predict(X=prediction_instance, verbose=True)\n",
    "nb_categorical.predict(X=[\"Self-emp-inc\", \"Doctorate\", \"White\", \"Male\"], verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, '?'),\n",
      "  (1, 'Federal-gov'),\n",
      "  (2, 'Local-gov'),\n",
      "  (3, 'Never-worked'),\n",
      "  (4, 'Private'),\n",
      "  (5, 'Self-emp-inc'),\n",
      "  (6, 'Self-emp-not-inc'),\n",
      "  (7, 'State-gov'),\n",
      "  (8, 'Without-pay')],\n",
      " [(0, '10th'),\n",
      "  (1, '11th'),\n",
      "  (2, '12th'),\n",
      "  (3, '1st-4th'),\n",
      "  (4, '5th-6th'),\n",
      "  (5, '7th-8th'),\n",
      "  (6, '9th'),\n",
      "  (7, 'Assoc-acdm'),\n",
      "  (8, 'Assoc-voc'),\n",
      "  (9, 'Bachelors'),\n",
      "  (10, 'Doctorate'),\n",
      "  (11, 'HS-grad'),\n",
      "  (12, 'Masters'),\n",
      "  (13, 'Preschool'),\n",
      "  (14, 'Prof-school'),\n",
      "  (15, 'Some-college')],\n",
      " [(0, 'Amer-Indian-Eskimo'),\n",
      "  (1, 'Asian-Pac-Islander'),\n",
      "  (2, 'Black'),\n",
      "  (3, 'Other'),\n",
      "  (4, 'White')],\n",
      " [(0, 'Female'), (1, 'Male')]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>workclass</th>\n",
       "      <th>education</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32556</th>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32557</th>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32558</th>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32559</th>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32560</th>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32561 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       workclass  education  race  gender income\n",
       "0              7          9     4       1  <=50K\n",
       "1              6          9     4       1  <=50K\n",
       "2              4         11     4       1  <=50K\n",
       "3              4          1     2       1  <=50K\n",
       "4              4          9     2       0  <=50K\n",
       "...          ...        ...   ...     ...    ...\n",
       "32556          4          7     4       0  <=50K\n",
       "32557          4         11     4       1   >50K\n",
       "32558          4         11     4       0  <=50K\n",
       "32559          4         11     4       1  <=50K\n",
       "32560          5         11     4       0   >50K\n",
       "\n",
       "[32561 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocess or transform the features in the income_evaluation_cat.csv data using an appropriate scaler in sklearn. You don’t need to transform the output variable; it should still work fine in a text format.\n",
    "le = LabelEncoder()\n",
    "\n",
    "encodings = []\n",
    "\n",
    "\n",
    "def encode_labels(encoder, X):\n",
    "    encoder.fit(X)\n",
    "    encodings.append(list(enumerate(encoder.classes_)))\n",
    "    return encoder.transform(X)\n",
    "\n",
    "\n",
    "income_evaluation_cat_encoded = income_evaluation_cat.drop(columns=\"income\").apply(lambda x: encode_labels(le, x))\n",
    "pp(encodings)\n",
    "income_evaluation_cat_encoded[\"income\"] = income_evaluation_cat[\"income\"]\n",
    "income_evaluation_cat_encoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly split the transformed input data and the output data into X_train, y_train, X_test and y_test using tools in sklearn.\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    income_evaluation_cat_encoded.drop(columns=[\"income\"]),\n",
    "    income_evaluation_cat_encoded[\"income\"],\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy 0.7838221744471745\n",
      "Test accuracy 0.7865806847842776\n"
     ]
    }
   ],
   "source": [
    "# Use an appropriate Naïve Bayes constructor in sklearn to construct and fit a Naïve Bayes model on the training data, then use the model to compute the accuracy score of the training and test set.\n",
    "cat_NB = CategoricalNB(fit_prior=True)\n",
    "cat_NB.fit(X_train, y_train)\n",
    "print(\"Training accuracy\", cat_NB.score(X_train, y_train))\n",
    "print(\"Test accuracy\", cat_NB.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, '?'),\n",
      "  (1, 'Federal-gov'),\n",
      "  (2, 'Local-gov'),\n",
      "  (3, 'Never-worked'),\n",
      "  (4, 'Private'),\n",
      "  (5, 'Self-emp-inc'),\n",
      "  (6, 'Self-emp-not-inc'),\n",
      "  (7, 'State-gov'),\n",
      "  (8, 'Without-pay')],\n",
      " [(0, '10th'),\n",
      "  (1, '11th'),\n",
      "  (2, '12th'),\n",
      "  (3, '1st-4th'),\n",
      "  (4, '5th-6th'),\n",
      "  (5, '7th-8th'),\n",
      "  (6, '9th'),\n",
      "  (7, 'Assoc-acdm'),\n",
      "  (8, 'Assoc-voc'),\n",
      "  (9, 'Bachelors'),\n",
      "  (10, 'Doctorate'),\n",
      "  (11, 'HS-grad'),\n",
      "  (12, 'Masters'),\n",
      "  (13, 'Preschool'),\n",
      "  (14, 'Prof-school'),\n",
      "  (15, 'Some-college')],\n",
      " [(0, 'Amer-Indian-Eskimo'),\n",
      "  (1, 'Asian-Pac-Islander'),\n",
      "  (2, 'Black'),\n",
      "  (3, 'Other'),\n",
      "  (4, 'White')],\n",
      " [(0, 'Female'), (1, 'Male')]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/raja_/Documents/GitHub/msds_winter_2022/venv/lib/python3.8/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but CategoricalNB was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/raja_/Documents/GitHub/msds_winter_2022/venv/lib/python3.8/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but CategoricalNB was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.05562795, 0.94437205]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp(encodings)\n",
    "cat_NB.predict_proba([[4, 9, 4, 0]])\n",
    "cat_NB.predict_proba([[5, 10, 4, 1]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>education_num</th>\n",
       "      <th>hours_per_week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>38.581647</td>\n",
       "      <td>10.080679</td>\n",
       "      <td>40.437456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>13.640433</td>\n",
       "      <td>2.572720</td>\n",
       "      <td>12.347429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            age  education_num  hours_per_week\n",
       "mean  38.581647      10.080679       40.437456\n",
       "std   13.640433       2.572720       12.347429"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Question 2\n",
    "# Upload the income_evaluation_continuous.csv data provided on canvas.  The features in this data include age, education_num, and hours_per_week. The output variable is income and contains two categorical values (<=50k or >50k) indicating whether the income of an individual is less than/equal to $50,000 or greater than $50,000 respectively. Compute the mean and standard deviation of each input variable such that the results are presented on the same table or data frame. You can call the .apply() function on the pandas DataFrame.\n",
    "income_evaluation_continuous = pd.read_csv(\"income_evaluation_continuous.csv\", skipinitialspace=True)\n",
    "income_stats = income_evaluation_continuous.agg([\"mean\", \"std\"])\n",
    "display(income_stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<=50K':                      mean        std\n",
       " age             36.783738  14.020088\n",
       " education_num    9.595065   2.436147\n",
       " hours_per_week  38.840210  12.318995,\n",
       " '>50K':                      mean        std\n",
       " age             44.249841  10.519028\n",
       " education_num   11.611657   2.385129\n",
       " hours_per_week  45.473026  11.012971}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Input: [30, 10, 45] \n",
      " Class Probabilities: {'<=50K': 0.834602773454588, '>50K': 0.16539722654541197} \n",
      " Predicted Class: <=50K\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<=50K'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You will implement Naïve Bayes from scratch using Bayes’ rule. Assume that all the features or input variables follow a normal distribution. You can do your calculations in Python, but you would not use the sklearn package. You can use the density function inside the stats module in the SciPy package. Given that all the income_evaluation_continuous.csv data you uploaded is the training data , classify a test instance, X = [30, 10, 45], into the class income<=50 or income>50k. You need to compute the posterior probabilities P(income<=50/X) and P(income>50k/X), then print the class with the greater posterior probability as the predicted class. You don’t need to define a function, but you could if you want.\n",
    "\n",
    "\n",
    "class NaiveBayesContinuous:\n",
    "    def __init__(\n",
    "        self,\n",
    "        data: pd.DataFrame = income_evaluation_continuous,\n",
    "        target_column: str = \"income\",\n",
    "    ):\n",
    "        self.data = data\n",
    "        self.target_column = target_column\n",
    "        self.unique_targets = self.data[self.target_column].unique()\n",
    "\n",
    "        self.stats_tables = self.calculate_stats()\n",
    "        self.target_value_likelihoods = self.data.groupby(target_column)[target_column].count() / len(self.data)\n",
    "\n",
    "    def calculate_stats(self):\n",
    "        stats_tables = {}\n",
    "        for target in self.unique_targets:\n",
    "            stats_tables[target] = self.data[self.data[self.target_column] == target].agg([\"mean\", \"std\"]).T\n",
    "        return stats_tables\n",
    "\n",
    "    def predict(self, X: list = None, verbose: bool = False):\n",
    "        probability_dict = {k: 1 for k in self.unique_targets}\n",
    "\n",
    "        for target, table in self.stats_tables.items():\n",
    "            table[\"X\"] = X\n",
    "            table[\"density\"] = table.apply(lambda x: norm.pdf(x[\"X\"], x[\"mean\"], x[\"std\"]), axis=1)\n",
    "\n",
    "            probability_dict[target] = table[\"density\"].prod() * self.target_value_likelihoods[target]\n",
    "\n",
    "        sum_probs = sum(probability_dict.values())\n",
    "        for target_class in probability_dict.keys():\n",
    "            probability_dict[target_class] /= sum_probs\n",
    "\n",
    "        predicted_class = max(probability_dict, key=probability_dict.get)\n",
    "        if verbose:\n",
    "            print(\n",
    "                \" Input:\",\n",
    "                X,\n",
    "                \"\\n\",\n",
    "                \"Class Probabilities:\",\n",
    "                probability_dict,\n",
    "                \"\\n\",\n",
    "                \"Predicted Class:\",\n",
    "                predicted_class,\n",
    "            )\n",
    "        return predicted_class\n",
    "\n",
    "\n",
    "nb_continuous = NaiveBayesContinuous()\n",
    "display(nb_continuous.stats_tables)\n",
    "nb_continuous.predict(X=[30, 10, 45], verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>education_num</th>\n",
       "      <th>hours_per_week</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>13</td>\n",
       "      <td>40</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>9</td>\n",
       "      <td>40</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>7</td>\n",
       "      <td>40</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>13</td>\n",
       "      <td>40</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32556</th>\n",
       "      <td>27</td>\n",
       "      <td>12</td>\n",
       "      <td>38</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32557</th>\n",
       "      <td>40</td>\n",
       "      <td>9</td>\n",
       "      <td>40</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32558</th>\n",
       "      <td>58</td>\n",
       "      <td>9</td>\n",
       "      <td>40</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32559</th>\n",
       "      <td>22</td>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32560</th>\n",
       "      <td>52</td>\n",
       "      <td>9</td>\n",
       "      <td>40</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32561 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       age  education_num  hours_per_week income\n",
       "0       39             13              40  <=50K\n",
       "1       50             13              13  <=50K\n",
       "2       38              9              40  <=50K\n",
       "3       53              7              40  <=50K\n",
       "4       28             13              40  <=50K\n",
       "...    ...            ...             ...    ...\n",
       "32556   27             12              38  <=50K\n",
       "32557   40              9              40   >50K\n",
       "32558   58              9              40  <=50K\n",
       "32559   22              9              20  <=50K\n",
       "32560   52              9              40   >50K\n",
       "\n",
       "[32561 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocess or transform the features in the income_evaluation_cont.csv data using an appropriate scaler in sklearn. You don’t need to transform the output variable; it should still work fine in a text format.\n",
    "income_evaluation_continuous\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly split the input and output data into X_train, y_train, X_test and y_test using tools in sklearn.\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    income_evaluation_continuous.drop(columns=\"income\"),\n",
    "    income_evaluation_continuous[\"income\"],\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy 0.7988329238329238\n",
      "Test accuracy 0.8000921234454169\n"
     ]
    }
   ],
   "source": [
    "# Use an appropriate Naïve Bayes constructor in sklearn to construct and fit a Naïve Bayes model on the training data, then use the model to compute the accuracy score of the training and test set.\n",
    "\n",
    "gaussian_NB = GaussianNB()\n",
    "gaussian_NB.fit(X_train, y_train)\n",
    "print(\"Training accuracy\", gaussian_NB.score(X_train, y_train))\n",
    "print(\"Test accuracy\", gaussian_NB.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>news_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>As U.S. budget fight looms, Republicans flip t...</td>\n",
       "      <td>WASHINGTON (Reuters) - The head of a conservat...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U.S. military to accept transgender recruits o...</td>\n",
       "      <td>WASHINGTON (Reuters) - Transgender people will...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Senior U.S. Republican senator: 'Let Mr. Muell...</td>\n",
       "      <td>WASHINGTON (Reuters) - The special counsel inv...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FBI Russia probe helped by Australian diplomat...</td>\n",
       "      <td>WASHINGTON (Reuters) - Trump campaign adviser ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Trump wants Postal Service to charge 'much mor...</td>\n",
       "      <td>SEATTLE/WASHINGTON (Reuters) - President Donal...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21412</th>\n",
       "      <td>'Fully committed' NATO backs new U.S. approach...</td>\n",
       "      <td>BRUSSELS (Reuters) - NATO allies on Tuesday we...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21413</th>\n",
       "      <td>LexisNexis withdrew two products from Chinese ...</td>\n",
       "      <td>LONDON (Reuters) - LexisNexis, a provider of l...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21414</th>\n",
       "      <td>Minsk cultural hub becomes haven from authorities</td>\n",
       "      <td>MINSK (Reuters) - In the shadow of disused Sov...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21415</th>\n",
       "      <td>Vatican upbeat on possibility of Pope Francis ...</td>\n",
       "      <td>MOSCOW (Reuters) - Vatican Secretary of State ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21416</th>\n",
       "      <td>Indonesia to buy $1.14 billion worth of Russia...</td>\n",
       "      <td>JAKARTA (Reuters) - Indonesia will buy 11 Sukh...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21417 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "0      As U.S. budget fight looms, Republicans flip t...   \n",
       "1      U.S. military to accept transgender recruits o...   \n",
       "2      Senior U.S. Republican senator: 'Let Mr. Muell...   \n",
       "3      FBI Russia probe helped by Australian diplomat...   \n",
       "4      Trump wants Postal Service to charge 'much mor...   \n",
       "...                                                  ...   \n",
       "21412  'Fully committed' NATO backs new U.S. approach...   \n",
       "21413  LexisNexis withdrew two products from Chinese ...   \n",
       "21414  Minsk cultural hub becomes haven from authorities   \n",
       "21415  Vatican upbeat on possibility of Pope Francis ...   \n",
       "21416  Indonesia to buy $1.14 billion worth of Russia...   \n",
       "\n",
       "                                                    text news_type  \n",
       "0      WASHINGTON (Reuters) - The head of a conservat...      True  \n",
       "1      WASHINGTON (Reuters) - Transgender people will...      True  \n",
       "2      WASHINGTON (Reuters) - The special counsel inv...      True  \n",
       "3      WASHINGTON (Reuters) - Trump campaign adviser ...      True  \n",
       "4      SEATTLE/WASHINGTON (Reuters) - President Donal...      True  \n",
       "...                                                  ...       ...  \n",
       "21412  BRUSSELS (Reuters) - NATO allies on Tuesday we...      True  \n",
       "21413  LONDON (Reuters) - LexisNexis, a provider of l...      True  \n",
       "21414  MINSK (Reuters) - In the shadow of disused Sov...      True  \n",
       "21415  MOSCOW (Reuters) - Vatican Secretary of State ...      True  \n",
       "21416  JAKARTA (Reuters) - Indonesia will buy 11 Sukh...      True  \n",
       "\n",
       "[21417 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Question 3:\n",
    "# You will now implement a Naïve Bayes for text classification to detect fake or true news.\n",
    "# Upload the True.csv data provided on canvas into Python. You will create a new data frame by selecting the “title” and “text” columns, then, adding a new column called “news_type” where all the values on this new column are “True”. So, your new data frame should have three columns; “title”, “text” and “news_type”.\n",
    "true_data = pd.read_csv(\"True.csv\", skipinitialspace=True)\n",
    "true_data = true_data[[\"title\", \"text\"]]\n",
    "true_data[\"news_type\"] = \"True\"\n",
    "true_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>news_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Donald Trump Sends Out Embarrassing New Year’s...</td>\n",
       "      <td>Donald Trump just couldn t wish all Americans ...</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Drunk Bragging Trump Staffer Started Russian C...</td>\n",
       "      <td>House Intelligence Committee Chairman Devin Nu...</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sheriff David Clarke Becomes An Internet Joke ...</td>\n",
       "      <td>On Friday, it was revealed that former Milwauk...</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trump Is So Obsessed He Even Has Obama’s Name ...</td>\n",
       "      <td>On Christmas day, Donald Trump announced that ...</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pope Francis Just Called Out Donald Trump Duri...</td>\n",
       "      <td>Pope Francis used his annual Christmas Day mes...</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23476</th>\n",
       "      <td>McPain: John McCain Furious That Iran Treated ...</td>\n",
       "      <td>21st Century Wire says As 21WIRE reported earl...</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23477</th>\n",
       "      <td>JUSTICE? Yahoo Settles E-mail Privacy Class-ac...</td>\n",
       "      <td>21st Century Wire says It s a familiar theme. ...</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23478</th>\n",
       "      <td>Sunnistan: US and Allied ‘Safe Zone’ Plan to T...</td>\n",
       "      <td>Patrick Henningsen  21st Century WireRemember ...</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23479</th>\n",
       "      <td>How to Blow $700 Million: Al Jazeera America F...</td>\n",
       "      <td>21st Century Wire says Al Jazeera America will...</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23480</th>\n",
       "      <td>10 U.S. Navy Sailors Held by Iranian Military ...</td>\n",
       "      <td>21st Century Wire says As 21WIRE predicted in ...</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23481 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "0      Donald Trump Sends Out Embarrassing New Year’s...   \n",
       "1      Drunk Bragging Trump Staffer Started Russian C...   \n",
       "2      Sheriff David Clarke Becomes An Internet Joke ...   \n",
       "3      Trump Is So Obsessed He Even Has Obama’s Name ...   \n",
       "4      Pope Francis Just Called Out Donald Trump Duri...   \n",
       "...                                                  ...   \n",
       "23476  McPain: John McCain Furious That Iran Treated ...   \n",
       "23477  JUSTICE? Yahoo Settles E-mail Privacy Class-ac...   \n",
       "23478  Sunnistan: US and Allied ‘Safe Zone’ Plan to T...   \n",
       "23479  How to Blow $700 Million: Al Jazeera America F...   \n",
       "23480  10 U.S. Navy Sailors Held by Iranian Military ...   \n",
       "\n",
       "                                                    text news_type  \n",
       "0      Donald Trump just couldn t wish all Americans ...      Fake  \n",
       "1      House Intelligence Committee Chairman Devin Nu...      Fake  \n",
       "2      On Friday, it was revealed that former Milwauk...      Fake  \n",
       "3      On Christmas day, Donald Trump announced that ...      Fake  \n",
       "4      Pope Francis used his annual Christmas Day mes...      Fake  \n",
       "...                                                  ...       ...  \n",
       "23476  21st Century Wire says As 21WIRE reported earl...      Fake  \n",
       "23477  21st Century Wire says It s a familiar theme. ...      Fake  \n",
       "23478  Patrick Henningsen  21st Century WireRemember ...      Fake  \n",
       "23479  21st Century Wire says Al Jazeera America will...      Fake  \n",
       "23480  21st Century Wire says As 21WIRE predicted in ...      Fake  \n",
       "\n",
       "[23481 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Upload the Fake.csv data provided on canvas into Python. You will create a new data frame by selecting the “title” and “text” columns, then, adding a new column called “news_type” where all the values on this new column are “Fake”. So, your new data frame should have three columns; “title”, “text” and “news_type”.\n",
    "fake_data = pd.read_csv(\"Fake.csv\", skipinitialspace=True)\n",
    "fake_data = fake_data[[\"title\", \"text\"]]\n",
    "fake_data[\"news_type\"] = \"Fake\"\n",
    "fake_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_type</th>\n",
       "      <th>news</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>As U.S. budget fight looms, Republicans flip t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>U.S. military to accept transgender recruits o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True</td>\n",
       "      <td>Senior U.S. Republican senator: 'Let Mr. Muell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True</td>\n",
       "      <td>FBI Russia probe helped by Australian diplomat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>True</td>\n",
       "      <td>Trump wants Postal Service to charge 'much mor...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  news_type                                               news\n",
       "0      True  As U.S. budget fight looms, Republicans flip t...\n",
       "1      True  U.S. military to accept transgender recruits o...\n",
       "2      True  Senior U.S. Republican senator: 'Let Mr. Muell...\n",
       "3      True  FBI Russia probe helped by Australian diplomat...\n",
       "4      True  Trump wants Postal Service to charge 'much mor..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Merge the data frame in a) and b) so that one of the data frames is stacked vertically on top of the other. Combine the text in the “title” and “text” columns of the merged data frame into another column called “news”. Drop the “title” and “text” columns so that your final data frame is has only two columns, “news” and “news_type”. Print the first five rows of your final data frame.\n",
    "merged_data = pd.concat([true_data, fake_data], ignore_index=True)\n",
    "merged_data[\"news\"] = merged_data[\"title\"] + \" \" + merged_data[\"text\"]\n",
    "merged_data.drop(columns=[\"title\", \"text\"], inplace=True)\n",
    "display(merged_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess your data by cleaning the textual data in the “news” column and removing the stop words, special characters, punctuations, etc especially at the beginning and end of each word. You can display any messy news text before you clean the data, then display the messy news text again after cleaning the data to see if your data cleaning worked well. Also, drop instances where the news text is less than 50 words.\n",
    "\n",
    "table = str.maketrans(\"\", \"\", string.punctuation)\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "\n",
    "def clean(text):\n",
    "    try:\n",
    "        tokens = word_tokenize(text)\n",
    "        stripped = [str(w).lower().translate(table) for w in tokens]\n",
    "        words = [word for word in stripped if word.isalpha()]\n",
    "        words = [w for w in words if not w in stop_words]\n",
    "        if len(words) > 50:\n",
    "            return \" \".join(words)\n",
    "        else:\n",
    "            return None\n",
    "    except TypeError:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Text\n",
      "As U.S. budget fight looms, Republicans flip their fiscal script WASHINGTON (Reuters) - The head of a conservative Republican faction in the U.S. Congress, who voted this month for a huge expansion of the national debt to pay for tax cuts, called himself a “fiscal conservative” on Sunday and urged budget restraint in 2018. In keeping with a sharp pivot under way among Republicans, U.S. Representative Mark Meadows, speaking on CBS’ “Face the Nation,” drew a hard line on federal spending, which lawmakers are bracing to do battle over in January. When they return from the holidays on Wednesday, lawmakers will begin trying to pass a federal budget in a fight likely to be linked to other issues, such as immigration policy, even as the November congressional election campaigns approach in which Republicans will seek to keep control of Congress. President Donald Trump and his Republicans want a big budget increase in military spending, while Democrats also want proportional increases for non-defense “discretionary” spending on programs that support education, scientific research, infrastructure, public health and environmental protection. “The (Trump) administration has already been willing to say: ‘We’re going to increase non-defense discretionary spending ... by about 7 percent,’” Meadows, chairman of the small but influential House Freedom Caucus, said on the program. “Now, Democrats are saying that’s not enough, we need to give the government a pay raise of 10 to 11 percent. For a fiscal conservative, I don’t see where the rationale is. ... Eventually you run out of other people’s money,” he said. Meadows was among Republicans who voted in late December for their party’s debt-financed tax overhaul, which is expected to balloon the federal budget deficit and add about $1.5 trillion over 10 years to the $20 trillion national debt. “It’s interesting to hear Mark talk about fiscal responsibility,” Democratic U.S. Representative Joseph Crowley said on CBS. Crowley said the Republican tax bill would require the  United States to borrow $1.5 trillion, to be paid off by future generations, to finance tax cuts for corporations and the rich. “This is one of the least ... fiscally responsible bills we’ve ever seen passed in the history of the House of Representatives. I think we’re going to be paying for this for many, many years to come,” Crowley said. Republicans insist the tax package, the biggest U.S. tax overhaul in more than 30 years,  will boost the economy and job growth. House Speaker Paul Ryan, who also supported the tax bill, recently went further than Meadows, making clear in a radio interview that welfare or “entitlement reform,” as the party often calls it, would be a top Republican priority in 2018. In Republican parlance, “entitlement” programs mean food stamps, housing assistance, Medicare and Medicaid health insurance for the elderly, poor and disabled, as well as other programs created by Washington to assist the needy. Democrats seized on Ryan’s early December remarks, saying they showed Republicans would try to pay for their tax overhaul by seeking spending cuts for social programs. But the goals of House Republicans may have to take a back seat to the Senate, where the votes of some Democrats will be needed to approve a budget and prevent a government shutdown. Democrats will use their leverage in the Senate, which Republicans narrowly control, to defend both discretionary non-defense programs and social spending, while tackling the issue of the “Dreamers,” people brought illegally to the country as children. Trump in September put a March 2018 expiration date on the Deferred Action for Childhood Arrivals, or DACA, program, which protects the young immigrants from deportation and provides them with work permits. The president has said in recent Twitter messages he wants funding for his proposed Mexican border wall and other immigration law changes in exchange for agreeing to help the Dreamers. Representative Debbie Dingell told CBS she did not favor linking that issue to other policy objectives, such as wall funding. “We need to do DACA clean,” she said.  On Wednesday, Trump aides will meet with congressional leaders to discuss those issues. That will be followed by a weekend of strategy sessions for Trump and Republican leaders on Jan. 6 and 7, the White House said. Trump was also scheduled to meet on Sunday with Florida Republican Governor Rick Scott, who wants more emergency aid. The House has passed an $81 billion aid package after hurricanes in Florida, Texas and Puerto Rico, and wildfires in California. The package far exceeded the $44 billion requested by the Trump administration. The Senate has not yet voted on the aid. \n",
      "Clean Text\n",
      "us budget fight looms republicans flip fiscal script washington reuters head conservative republican faction us congress voted month huge expansion national debt pay tax cuts called fiscal conservative sunday urged budget restraint keeping sharp pivot way among republicans us representative mark meadows speaking cbs face nation drew hard line federal spending lawmakers bracing battle january return holidays wednesday lawmakers begin trying pass federal budget fight likely linked issues immigration policy even november congressional election campaigns approach republicans seek keep control congress president donald trump republicans want big budget increase military spending democrats also want proportional increases nondefense discretionary spending programs support education scientific research infrastructure public health environmental protection trump administration already willing say going increase nondefense discretionary spending percent meadows chairman small influential house freedom caucus said program democrats saying enough need give government pay raise percent fiscal conservative see rationale eventually run people money said meadows among republicans voted late december party debtfinanced tax overhaul expected balloon federal budget deficit add trillion years trillion national debt interesting hear mark talk fiscal responsibility democratic us representative joseph crowley said cbs crowley said republican tax bill would require united states borrow trillion paid future generations finance tax cuts corporations rich one least fiscally responsible bills ever seen passed history house representatives think going paying many many years come crowley said republicans insist tax package biggest us tax overhaul years boost economy job growth house speaker paul ryan also supported tax bill recently went meadows making clear radio interview welfare entitlement reform party often calls would top republican priority republican parlance entitlement programs mean food stamps housing assistance medicare medicaid health insurance elderly poor disabled well programs created washington assist needy democrats seized ryan early december remarks saying showed republicans would try pay tax overhaul seeking spending cuts social programs goals house republicans may take back seat senate votes democrats needed approve budget prevent government shutdown democrats use leverage senate republicans narrowly control defend discretionary nondefense programs social spending tackling issue dreamers people brought illegally country children trump september put march expiration date deferred action childhood arrivals daca program protects young immigrants deportation provides work permits president said recent twitter messages wants funding proposed mexican border wall immigration law changes exchange agreeing help dreamers representative debbie dingell told cbs favor linking issue policy objectives wall funding need daca clean said wednesday trump aides meet congressional leaders discuss issues followed weekend strategy sessions trump republican leaders jan white house said trump also scheduled meet sunday florida republican governor rick scott wants emergency aid house passed billion aid package hurricanes florida texas puerto rico wildfires california package far exceeded billion requested trump administration senate yet voted aid\n"
     ]
    }
   ],
   "source": [
    "print(\"Raw Text\")\n",
    "print(merged_data[\"news\"][0])\n",
    "\n",
    "print(\"Clean Text\")\n",
    "print(clean(merged_data[\"news\"][0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 16 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_type</th>\n",
       "      <th>news</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>us budget fight looms republicans flip fiscal ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>us military accept transgender recruits monday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True</td>\n",
       "      <td>senior us republican senator let mr mueller jo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True</td>\n",
       "      <td>fbi russia probe helped australian diplomat ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>True</td>\n",
       "      <td>trump wants postal service charge much amazon ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44893</th>\n",
       "      <td>Fake</td>\n",
       "      <td>mcpain john mccain furious iran treated us sai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44894</th>\n",
       "      <td>Fake</td>\n",
       "      <td>justice yahoo settles email privacy classactio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44895</th>\n",
       "      <td>Fake</td>\n",
       "      <td>sunnistan us allied safe zone plan take territ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44896</th>\n",
       "      <td>Fake</td>\n",
       "      <td>blow million al jazeera america finally calls ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44897</th>\n",
       "      <td>Fake</td>\n",
       "      <td>us navy sailors held iranian military signs ne...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44898 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      news_type                                               news\n",
       "0          True  us budget fight looms republicans flip fiscal ...\n",
       "1          True  us military accept transgender recruits monday...\n",
       "2          True  senior us republican senator let mr mueller jo...\n",
       "3          True  fbi russia probe helped australian diplomat ti...\n",
       "4          True  trump wants postal service charge much amazon ...\n",
       "...         ...                                                ...\n",
       "44893      Fake  mcpain john mccain furious iran treated us sai...\n",
       "44894      Fake  justice yahoo settles email privacy classactio...\n",
       "44895      Fake  sunnistan us allied safe zone plan take territ...\n",
       "44896      Fake  blow million al jazeera america finally calls ...\n",
       "44897      Fake  us navy sailors held iranian military signs ne...\n",
       "\n",
       "[44898 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandarallel.initialize()\n",
    "merged_data[\"news\"] = merged_data[\"news\"].parallel_apply(clean)\n",
    "merged_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.0980796 , 0.        , ..., 0.        , 0.        ,\n",
       "        0.1279775 ],\n",
       "       [0.        , 0.52997391, 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.11903907, 0.        , ..., 0.        , 0.05109624,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.05670896, 0.05995031, 0.05854585, ..., 0.08964584, 0.02573302,\n",
       "        0.1564503 ],\n",
       "       [0.        , 0.        , 0.48992678, ..., 0.        , 0.08613624,\n",
       "        0.        ],\n",
       "       [0.27782281, 0.        , 0.        , ..., 0.        , 0.06303432,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform the input text data into feature vectors where the entries of the feature vectors are term-frequency-inverse-document-frequency. Use the TfidfVectorizer() in sklearn.\n",
    "tfidf = TfidfVectorizer(ngram_range=(1, 2), stop_words=\"english\", decode_error=\"ignore\", max_features=100)\n",
    "\n",
    "merged_data = merged_data[merged_data[\"news\"].notnull()]\n",
    "news_tfidf = tfidf.fit_transform(merged_data[\"news\"])\n",
    "news_tfidf.toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spit the feature vectors and the output variable into into X_train, y_train, X_test and y_test, you can let the test set be 30% of the entire data.\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    news_tfidf.toarray(), merged_data[\"news_type\"], test_size=0.3, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy 0.9042817679558011\n",
      "Test accuracy 0.9007412181759588\n"
     ]
    }
   ],
   "source": [
    "# Fit an appropriate Naïve Bayes model and compute the training and test accuracy of the model. Is there overfitting?\n",
    "multinomial_NB = MultinomialNB()\n",
    "multinomial_NB.fit(X_train, y_train)\n",
    "print(\"Training accuracy\", multinomial_NB.score(X_train, y_train))\n",
    "print(\"Test accuracy\", multinomial_NB.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average cross validation score 0.8987470744511761\n",
      "STD cross validation score 0.0552014938734187\n"
     ]
    }
   ],
   "source": [
    "# Fit a Naïve Bayes using cross validation and print the average cross validation score as well as the standard deviation of the cross-validation scores.\n",
    "multinomial_nb_cross_val_scores = cross_val_score(\n",
    "    multinomial_NB, news_tfidf.toarray(), merged_data[\"news_type\"], cv=31\n",
    ")\n",
    "print(\"Average cross validation score\", multinomial_nb_cross_val_scores.mean())\n",
    "print(\"STD cross validation score\", multinomial_nb_cross_val_scores.std())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average cross validation score 0.8907206920816085\n",
      "STD cross validation score 0.04920878104823473\n"
     ]
    }
   ],
   "source": [
    "# Select some hypermeters of your choice and tune using the grid search cross validation. Use some other hyperparameters than those used in class examples.\n",
    "\n",
    "multinomial_NB_grid = GridSearchCV(MultinomialNB(), {\"alpha\": [0.1, 1, 10, 100]}, cv=10, n_jobs=-1)\n",
    "\n",
    "multinomial_NB_grid_cross_val_scores = cross_val_score(\n",
    "    multinomial_NB_grid, news_tfidf.toarray(), merged_data[\"news_type\"], cv=10, n_jobs=-1\n",
    ")\n",
    "print(\"Average cross validation score\", multinomial_NB_grid_cross_val_scores.mean())\n",
    "print(\"STD cross validation score\", multinomial_NB_grid_cross_val_scores.std())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14640 entries, 0 to 14639\n",
      "Data columns (total 15 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   tweet_id                      14640 non-null  int64  \n",
      " 1   airline_sentiment             14640 non-null  object \n",
      " 2   airline_sentiment_confidence  14640 non-null  float64\n",
      " 3   negativereason                9178 non-null   object \n",
      " 4   negativereason_confidence     10522 non-null  float64\n",
      " 5   airline                       14640 non-null  object \n",
      " 6   airline_sentiment_gold        40 non-null     object \n",
      " 7   name                          14640 non-null  object \n",
      " 8   negativereason_gold           32 non-null     object \n",
      " 9   retweet_count                 14640 non-null  int64  \n",
      " 10  text                          14640 non-null  object \n",
      " 11  tweet_coord                   1019 non-null   object \n",
      " 12  tweet_created                 14640 non-null  object \n",
      " 13  tweet_location                9907 non-null   object \n",
      " 14  user_timezone                 9820 non-null   object \n",
      "dtypes: float64(2), int64(2), object(11)\n",
      "memory usage: 1.7+ MB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570306133677760513</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570301083672813571</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:48 -0800</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570301031407624196</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:36 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570300817074462722</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:45 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0  570306133677760513           neutral                        1.0000   \n",
       "1  570301130888122368          positive                        0.3486   \n",
       "2  570301083672813571           neutral                        0.6837   \n",
       "3  570301031407624196          negative                        1.0000   \n",
       "4  570300817074462722          negative                        1.0000   \n",
       "\n",
       "  negativereason  negativereason_confidence         airline  \\\n",
       "0            NaN                        NaN  Virgin America   \n",
       "1            NaN                     0.0000  Virgin America   \n",
       "2            NaN                        NaN  Virgin America   \n",
       "3     Bad Flight                     0.7033  Virgin America   \n",
       "4     Can't Tell                     1.0000  Virgin America   \n",
       "\n",
       "  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n",
       "0                    NaN     cairdin                 NaN              0   \n",
       "1                    NaN    jnardino                 NaN              0   \n",
       "2                    NaN  yvonnalynn                 NaN              0   \n",
       "3                    NaN    jnardino                 NaN              0   \n",
       "4                    NaN    jnardino                 NaN              0   \n",
       "\n",
       "                                                text tweet_coord  \\\n",
       "0                @VirginAmerica What @dhepburn said.         NaN   \n",
       "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
       "2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
       "3  @VirginAmerica it's really aggressive to blast...         NaN   \n",
       "4  @VirginAmerica and it's a really big bad thing...         NaN   \n",
       "\n",
       "               tweet_created tweet_location               user_timezone  \n",
       "0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)  \n",
       "1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)  \n",
       "2  2015-02-24 11:15:48 -0800      Lets Play  Central Time (US & Canada)  \n",
       "3  2015-02-24 11:15:36 -0800            NaN  Pacific Time (US & Canada)  \n",
       "4  2015-02-24 11:14:45 -0800            NaN  Pacific Time (US & Canada)  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4) Mini Project\n",
    "# Find some text data of your own choice, it could be labelled tweets, etc.\n",
    "# Your dataset should have at least 200 instances, and if there are several columns of text, you can choose to merge the text columns into a single text column. Each text instance should have at least 60 words.\n",
    "tweets_data = pd.read_csv(\"Tweets.csv\", skipinitialspace=True)\n",
    "print(tweets_data.info())\n",
    "tweets_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570306133677760513</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570301083672813571</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:48 -0800</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570301031407624196</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:36 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570300817074462722</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:45 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0  570306133677760513           neutral                        1.0000   \n",
       "1  570301130888122368          positive                        0.3486   \n",
       "2  570301083672813571           neutral                        0.6837   \n",
       "3  570301031407624196          negative                        1.0000   \n",
       "4  570300817074462722          negative                        1.0000   \n",
       "\n",
       "  negativereason  negativereason_confidence         airline  \\\n",
       "0            NaN                        NaN  Virgin America   \n",
       "1            NaN                     0.0000  Virgin America   \n",
       "2            NaN                        NaN  Virgin America   \n",
       "3     Bad Flight                     0.7033  Virgin America   \n",
       "4     Can't Tell                     1.0000  Virgin America   \n",
       "\n",
       "  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n",
       "0                    NaN     cairdin                 NaN              0   \n",
       "1                    NaN    jnardino                 NaN              0   \n",
       "2                    NaN  yvonnalynn                 NaN              0   \n",
       "3                    NaN    jnardino                 NaN              0   \n",
       "4                    NaN    jnardino                 NaN              0   \n",
       "\n",
       "                                                text tweet_coord  \\\n",
       "0                @VirginAmerica What @dhepburn said.         NaN   \n",
       "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
       "2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
       "3  @VirginAmerica it's really aggressive to blast...         NaN   \n",
       "4  @VirginAmerica and it's a really big bad thing...         NaN   \n",
       "\n",
       "               tweet_created tweet_location               user_timezone  \n",
       "0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)  \n",
       "1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)  \n",
       "2  2015-02-24 11:15:48 -0800      Lets Play  Central Time (US & Canada)  \n",
       "3  2015-02-24 11:15:36 -0800            NaN  Pacific Time (US & Canada)  \n",
       "4  2015-02-24 11:14:45 -0800            NaN  Pacific Time (US & Canada)  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14640 entries, 0 to 14639\n",
      "Data columns (total 6 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   airline_sentiment  14640 non-null  object\n",
      " 1   airline            14640 non-null  object\n",
      " 2   negativereason     9178 non-null   object\n",
      " 3   text               14640 non-null  object\n",
      " 4   tweet_location     9907 non-null   object\n",
      " 5   user_timezone      9820 non-null   object\n",
      "dtypes: object(6)\n",
      "memory usage: 686.4+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy 0.89032006245121\n",
      "Test accuracy 0.8879781420765027\n"
     ]
    }
   ],
   "source": [
    "# 4) Mini Project\n",
    "# Find some text data of your own choice, it could be labelled tweets, etc.\n",
    "# Your dataset should have at least 200 instances, and if there are several columns of text, you can choose to merge the text columns into a single text column. Each text instance should have at least 60 words.\n",
    "tweets_data = pd.read_csv(\"Tweets.csv\", skipinitialspace=True)\n",
    "display(tweets_data.head())\n",
    "\n",
    "# Clean the data, split the data, transform the data to a representation suitable for your algorithm, build your model and evaluate the model.\n",
    "tweets_data = tweets_data[[\"airline_sentiment\", \"airline\", \"negativereason\", \"text\", \"tweet_location\", \"user_timezone\"]]\n",
    "display(tweets_data.info())\n",
    "tweets_data[\"review\"] = (\n",
    "    tweets_data[\"airline\"].astype(str)\n",
    "    + \" \" \n",
    "    + tweets_data[\"negativereason\"].astype(str)\n",
    "    + \" \" \n",
    "    + tweets_data[\"text\"].astype(str)\n",
    "    + \" \"\n",
    "    + tweets_data[\"tweet_location\"].astype(str)\n",
    "    + \" \"\n",
    "    + tweets_data[\"user_timezone\"].astype(str)\n",
    ")\n",
    "tweets_data[\"review\"] = tweets_data[\"review\"].str.replace(\"nan\", \"\")\n",
    "\n",
    "\n",
    "def clean_tweet(text):\n",
    "    try:\n",
    "        tokens = word_tokenize(text)\n",
    "        stripped = [str(w).lower().translate(table) for w in tokens]\n",
    "        words = [word for word in stripped if word.isalpha()]\n",
    "        words = [w for w in words if not w in stop_words]\n",
    "        return \" \".join(words)\n",
    "    except TypeError:\n",
    "        return None\n",
    "\n",
    "\n",
    "tweets_data[\"review\"] = tweets_data[\"review\"].parallel_apply(clean_tweet)\n",
    "tweets_data.drop(columns=[\"negativereason\", \"text\",\"airline\", \"tweet_location\", \"user_timezone\"], inplace=True)\n",
    "tweets_data\n",
    "\n",
    "tweet_tfidf = TfidfVectorizer(ngram_range=(1, 2), stop_words=\"english\", decode_error=\"ignore\", max_features=100)\n",
    "tweet_tfidf_matrix = tweet_tfidf.fit_transform(tweets_data[\"review\"])\n",
    "\n",
    "X_tweet_train, X_tweet_test, y_tweet_train, y_tweet_test = train_test_split(\n",
    "    tweet_tfidf_matrix.toarray(), tweets_data[\"airline_sentiment\"], test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Tune some parameters of interest\n",
    "multinomial_NB_tweet = GridSearchCV(MultinomialNB(), {\"alpha\": [0.1, 1, 10, 100]}, cv=10, n_jobs=-1)\n",
    "multinomial_NB_tweet.fit(X_tweet_train, y_tweet_train)\n",
    "\n",
    "print(\"Training accuracy\", multinomial_NB_tweet.score(X_tweet_train, y_tweet_train))\n",
    "print(\"Test accuracy\", multinomial_NB_tweet.score(X_tweet_test, y_tweet_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 0.1}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multinomial_NB_tweet.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bd88c1f76f7356ca69ef3da9f14355d0cc834d294444a1e290947aef5ce186b6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 ('venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
